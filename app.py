import streamlit as st
import os
import tempfile
import time
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

# Core RAG Components
from src.helper import download_embeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage 
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Loaders
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredExcelLoader

# 1. PAGE CONFIG & CACHING
st.set_page_config(page_title="Nexus Intelligence System", layout="wide")
load_dotenv()

@st.cache_resource
def get_embeddings():
Â  Â  """Caches the embedding model to RAM to prevent slow reloads."""
Â  Â  return download_embeddings()

# --- CLOUD-SAFE API KEY LOGIC ---
try:
Â  Â  groq_key = st.secrets["GROQ_API_KEY"]
except:
Â  Â  groq_key = os.getenv("GROQ_API_KEY")

# 2. SESSION STATE INITIALIZATION
if "messages" not in st.session_state: st.session_state.messages = []
if "chat_history" not in st.session_state: st.session_state.chat_history = []
if "vector_db" not in st.session_state: st.session_state.vector_db = None
if "logs" not in st.session_state: st.session_state.logs = []
if "total_chunks" not in st.session_state: st.session_state.total_chunks = 0
if "sync_time" not in st.session_state: st.session_state.sync_time = 0.0

def add_log(text):
Â  Â  timestamp = time.strftime("%H:%M:%S")
Â  Â  st.session_state.logs.append(f"[{timestamp}] {text}")

def load_single_file(uploaded_file):
Â  Â  suffix = f".{uploaded_file.name.split('.')[-1]}"
Â  Â  with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
Â  Â  Â  Â  tmp.write(uploaded_file.getvalue())
Â  Â  Â  Â  tmp_path = tmp.name
Â  Â  
Â  Â  if uploaded_file.name.endswith(".pdf"): loader = PyPDFLoader(tmp_path)
Â  Â  elif uploaded_file.name.endswith(".docx"): loader = Docx2txtLoader(tmp_path)
Â  Â  elif uploaded_file.name.endswith(".xlsx"): loader = UnstructuredExcelLoader(tmp_path)
Â  Â  
Â  Â  docs = loader.load()
Â  Â  for doc in docs: doc.metadata["source_file"] = uploaded_file.name
Â  Â  os.remove(tmp_path)
Â  Â  return docs

# 3. SIDEBAR: INTELLIGENCE MODES & SYSTEM CONTROLS
with st.sidebar:
Â  Â  st.title("Nexus Control")
Â  Â  
Â  Â  # INTUITIVE MODE SELECTION
Â  Â  st.subheader("ğŸ§  Intelligence Mode")
Â  Â  mode = st.radio(
Â  Â  Â  Â  "Select Operation Mode:",
Â  Â  Â  Â  ["Instant", "General", "Research", "Deep Think"],
Â  Â  Â  Â  index=1
Â  Â  )

Â  Â  # DYNAMIC LOGIC FOR MODES
Â  Â  mode_data = {
Â  Â  Â  Â  "Instant": {"temp": 0.7, "k": 3, "desc": "âš¡ **Instant Mode:** Prioritizes speed. Uses 3 chunks for a fast, conversational response."},
Â  Â  Â  Â  "General": {"temp": 0.4, "k": 5, "desc": "âš–ï¸ **General Mode:** The balanced zone. Uses 5 chunks for reliable daily tasks."},
Â  Â  Â  Â  "Research": {"temp": 0.2, "k": 10, "desc": "ğŸ” **Research Mode:** Deep retrieval. Grabs 10 chunks to ensure no detail is missed."},
Â  Â  Â  Â  "Deep Think": {"temp": 0.1, "k": 15, "desc": "ğŸ§  **Deep Think:** Maximum accuracy. Cross-references 15 nodes for logical synthesis."}
Â  Â  }
Â  Â  
Â  Â  st.info(mode_data[mode]["desc"])
Â  Â  
Â  Â  # Apply parameters
Â  Â  current_temp = mode_data[mode]["temp"]
Â  Â  current_k = mode_data[mode]["k"]

Â  Â  st.divider()
Â  Â  if st.button("Purge Session"):
Â  Â  Â  Â  st.session_state.messages, st.session_state.chat_history, st.session_state.logs, st.session_state.total_chunks, st.session_state.vector_db = [], [], [], 0, None
Â  Â  Â  Â  st.rerun()

Â  Â  st.divider()
Â  Â  st.subheader("ğŸ“ Data Ingestion")
Â  Â  uploaded_files = st.file_uploader("Upload Tech Docs", type=["pdf", "docx", "xlsx"], accept_multiple_files=True)
Â  Â  
Â  Â  if st.button("Initialize Sync"):
Â  Â  Â  Â  if uploaded_files:
Â  Â  Â  Â  Â  Â  start_time = time.time()
Â  Â  Â  Â  Â  Â  add_log(f"Sync initiated for {len(uploaded_files)} files...")
Â  Â  Â  Â  Â  Â  with ThreadPoolExecutor() as executor:
Â  Â  Â  Â  Â  Â  Â  Â  results = list(executor.map(load_single_file, uploaded_files))
Â  Â  Â  Â  Â  Â  all_docs = [doc for sublist in results for doc in sublist]
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=150)
Â  Â  Â  Â  Â  Â  final_docs = text_splitter.split_documents(all_docs)
Â  Â  Â  Â  Â  Â  st.session_state.total_chunks = len(final_docs)
Â  Â  Â  Â  Â  Â  st.session_state.vector_db = FAISS.from_documents(final_docs, get_embeddings())
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  st.session_state.sync_time = time.time() - start_time
Â  Â  Â  Â  Â  Â  add_log(f"Sync successful: {st.session_state.sync_time:.2f}s")
Â  Â  Â  Â  Â  Â  st.success(f"Sync Complete: {st.session_state.sync_time:.2f}s")

Â  Â  # METRICS DISPLAY
Â  Â  st.divider()
Â  Â  st.markdown(f"**Last Sync:** `{st.session_state.sync_time:.2f}s` | **Nodes:** `{st.session_state.total_chunks}`")
Â  Â  
Â  Â  st.subheader("ğŸ“Ÿ System Logs")
Â  Â  for log in st.session_state.logs[-5:]:
Â  Â  Â  Â  st.markdown(f"<p style='font-size:11px; color:#888; margin:0;'>{log}</p>", unsafe_allow_html=True)

# 4. MAIN CHAT INTERFACE
st.title("Nexus Intelligence Agent")
st.caption(f"Status: **{mode} Mode** is engaged.")

for message in st.session_state.messages:
Â  Â  with st.chat_message(message["role"]):
Â  Â  Â  Â  st.markdown(message["content"])

# 5. RETRIEVAL & RESPONSE LOGIC
if prompt := st.chat_input("Query the knowledge base..."):
Â  Â  st.session_state.messages.append({"role": "user", "content": prompt})
Â  Â  with st.chat_message("user"): st.markdown(prompt)

Â  Â  with st.chat_message("assistant"):
Â  Â  Â  Â  if not st.session_state.vector_db:
Â  Â  Â  Â  Â  Â  st.warning("Nexus requires data. Please upload and sync documents.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  # START ANALYSIS TIMER
Â  Â  Â  Â  Â  Â  Â  Â  analysis_start = time.time()
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  llm = ChatGroq(model_name="llama-3.3-70b-versatile", groq_api_key=groq_key, temperature=current_temp)
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  # Dynamic System Prompting
Â  Â  Â  Â  Â  Â  Â  Â  system_instruction = "You are the Nexus Technical Agent. "
Â  Â  Â  Â  Â  Â  Â  Â  if mode == "Deep Think":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  system_instruction += "Think step-by-step. Analyze all provided context for complex links and contradictions."
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  prompt_template = ChatPromptTemplate.from_messages([
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("system", f"{system_instruction}\n\nContext:\n{{context}}"),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  MessagesPlaceholder(variable_name="chat_history"),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("human", "{input}"),
Â  Â  Â  Â  Â  Â  Â  Â  ])

Â  Â  Â  Â  Â  Â  Â  Â  # Use dynamic K value based on mode
Â  Â  Â  Â  Â  Â  Â  Â  retriever = st.session_state.vector_db.as_retriever(search_kwargs={"k": current_k})
Â  Â  Â  Â  Â  Â  Â  Â  document_chain = create_stuff_documents_chain(llm, prompt_template)
Â  Â  Â  Â  Â  Â  Â  Â  retrieval_chain = create_retrieval_chain(retriever, document_chain)

Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(f"Nexus {mode} is analyzing..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response = retrieval_chain.invoke({"input": prompt, "chat_history": st.session_state.chat_history})

Â  Â  Â  Â  Â  Â  Â  Â  # CALCULATE DURATION
Â  Â  Â  Â  Â  Â  Â  Â  analysis_duration = time.time() - analysis_start
Â  Â  Â  Â  Â  Â  Â  Â  ans = response["answer"]
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  # OUTPUT DISPLAY
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(ans)
Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"â±ï¸ Analysis Time: **{analysis_duration:.2f}s** | Mode: **{mode}**")
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  with st.expander("ğŸ” Intelligence Evidence (Source Attribution)"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for i, doc in enumerate(response["context"]):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  source = doc.metadata.get("source_file", "Unknown")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page = doc.metadata.get("page", "N/A")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**{i+1}. {source}** (Pg. {page})")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"{doc.page_content[:150]}...")
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history.extend([HumanMessage(content=prompt), AIMessage(content=ans)])
Â  Â  Â  Â  Â  Â  Â  Â  if len(st.session_state.chat_history) > 10: st.session_state.chat_history = st.session_state.chat_history[-10:]
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.messages.append({"role": "assistant", "content": ans})

Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Execution Error: {e}")